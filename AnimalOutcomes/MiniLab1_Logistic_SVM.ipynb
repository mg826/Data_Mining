{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Lab 1: Logistic Regression and Support Vector Machines\n",
    "## Animal Shelter Outcomes: A Classification Problem\n",
    "### Alex Matsunami, RJ Smith, Cory Nichols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Project 1\n",
    "Various transformations and explorations were performed in Project 1, and data saved to CSV as the conclusion of that project.\n",
    "\n",
    "Here, we load in that data. For more information, please refer to the Project 1 notebook at https://github.com/pn1012/Data_Mining/blob/master/AnimalOutcomes/Project1_Exploration_Visualization_Dreduce.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,Imputer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26728, 27)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Project1.csv\")\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Lab Start : Logistic Regression and Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on exploratory data analysis, some initial conclusions were made: we could easily see that fixing animals has a significant effect on their ability to be adopted, simply by visualizing the data and running basic statistical tests. \n",
    "\n",
    "However, we figured we were missing a few variables. In particular, the time of the outcome. Understanding when outcomes happen could be a critical component in understanding what optimal times or days our business partners should stay open or promote adoptive benefits heavily. \n",
    "\n",
    "Further, in order to answer the question more succintly, we have reduced our problem to a binary one: adopted or not. Given a binary outcome, there is roughly a 45/55 split on adopted and not adopted outcomes, respectively. \n",
    "\n",
    "A binary outcome will help to explain classification models more effectively and answer our business partner's question more directly: what variables are the most important for ADOPTION?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is get our data into algorithmic shape. We intend to use two methodologies to build a classification model predicting binary outcomes. These two methodologies are Logistic Regression and Support Vector Machines.\n",
    "\n",
    "Given the linear separation dependency of Logistic Regression, we find it critical to attempt a non-linear classification algorithm with a Support Vector Machine. We will also fit a linear SVM to the data as well.\n",
    "\n",
    "This should help us to not only fit a predictive model, but to also create a description for the answer to our the question: \"How can we get more animals adopted?\"\n",
    "\n",
    "Let's go ahead and clean our data up. We have quite a few features left over, simply for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Type</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "      <th>Has_Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Is_Weekday</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>age_range</th>\n",
       "      <th>dark</th>\n",
       "      <th>exotic</th>\n",
       "      <th>light</th>\n",
       "      <th>neutral</th>\n",
       "      <th>Outcome_Label</th>\n",
       "      <th>Primary_Color_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hambone</td>\n",
       "      <td>2014-02-13 02:22:00</td>\n",
       "      <td>Return_to_owner</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "      <td>Brown/White</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>a_young</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Emily</td>\n",
       "      <td>2013-10-13 19:44:00</td>\n",
       "      <td>Euthanasia</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Cream Tabby</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>a_young</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pearce</td>\n",
       "      <td>2015-01-31 20:28:00</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "      <td>Blue/White</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>b_adult</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-12 02:09:00</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Blue Cream</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>a_young</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-15 20:52:00</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "      <td>Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>b_adult</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Name             DateTime          Outcome Type  \\\n",
       "0           0  Hambone  2014-02-13 02:22:00  Return_to_owner  Dog   \n",
       "1           1    Emily  2013-10-13 19:44:00       Euthanasia  Cat   \n",
       "2           2   Pearce  2015-01-31 20:28:00         Adoption  Dog   \n",
       "3           3      NaN  2014-07-12 02:09:00         Transfer  Cat   \n",
       "4           4      NaN  2013-11-15 20:52:00         Transfer  Dog   \n",
       "\n",
       "  SexuponOutcome AgeuponOutcome                        Breed        Color  \\\n",
       "0  Neutered Male         1 year        Shetland Sheepdog Mix  Brown/White   \n",
       "1  Spayed Female         1 year       Domestic Shorthair Mix  Cream Tabby   \n",
       "2  Neutered Male        2 years                 Pit Bull Mix   Blue/White   \n",
       "3    Intact Male        3 weeks       Domestic Shorthair Mix   Blue Cream   \n",
       "4  Neutered Male        2 years  Lhasa Apso/Miniature Poodle          Tan   \n",
       "\n",
       "   Has_Name         ...          Is_Weekday time_of_day  hour  age_range dark  \\\n",
       "0         1         ...                   1           0     2    a_young  0.0   \n",
       "1         1         ...                   0           2    19    a_young  0.0   \n",
       "2         1         ...                   0           2    20    b_adult  0.0   \n",
       "3         0         ...                   0           0     2    a_young  0.0   \n",
       "4         0         ...                   1           2    20    b_adult  0.0   \n",
       "\n",
       "   exotic  light  neutral  Outcome_Label  Primary_Color_Label  \n",
       "0     0.0    0.0      1.0              3                    3  \n",
       "1     0.0    1.0      0.0              2                    2  \n",
       "2     0.0    1.0      0.0              0                    2  \n",
       "3     0.0    1.0      0.0              4                    2  \n",
       "4     0.0    0.0      1.0              4                    3  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a new dataframe called df_alg (for df_algorithm), and discretize our mostly categorical dataset. \n",
    "\n",
    "We've also included our new time features for good measure: time_of_day, is_weekday, and hour. These variables are ordinal, binary and continuous in nature, they do not require dummies and the range on hour is 24. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adoption 10769\n",
      "Died 197\n",
      "Euthanasia 1555\n",
      "Return_to_owner 4785\n",
      "Transfer 9422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:461: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26728, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Fixed</th>\n",
       "      <th>Has_Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Is_Aggressive</th>\n",
       "      <th>Size</th>\n",
       "      <th>Is_Popular</th>\n",
       "      <th>Age_Value</th>\n",
       "      <th>Gender</th>\n",
       "      <th>dark</th>\n",
       "      <th>exotic</th>\n",
       "      <th>light</th>\n",
       "      <th>neutral</th>\n",
       "      <th>Is_Weekday</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>Outcome_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "      <td>26728.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.723661</td>\n",
       "      <td>0.712249</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.162975</td>\n",
       "      <td>0.624626</td>\n",
       "      <td>0.188043</td>\n",
       "      <td>794.037227</td>\n",
       "      <td>0.524656</td>\n",
       "      <td>0.317532</td>\n",
       "      <td>0.099559</td>\n",
       "      <td>0.289547</td>\n",
       "      <td>0.293363</td>\n",
       "      <td>0.695825</td>\n",
       "      <td>1.144904</td>\n",
       "      <td>12.767996</td>\n",
       "      <td>0.402911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.447195</td>\n",
       "      <td>0.452723</td>\n",
       "      <td>0.492999</td>\n",
       "      <td>0.369350</td>\n",
       "      <td>0.902864</td>\n",
       "      <td>0.390753</td>\n",
       "      <td>1082.374554</td>\n",
       "      <td>0.499401</td>\n",
       "      <td>0.465525</td>\n",
       "      <td>0.299416</td>\n",
       "      <td>0.453560</td>\n",
       "      <td>0.455312</td>\n",
       "      <td>0.460066</td>\n",
       "      <td>0.951823</td>\n",
       "      <td>9.471827</td>\n",
       "      <td>0.490492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1095.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Is_Fixed      Has_Name          Type  Is_Aggressive          Size  \\\n",
       "count  26728.000000  26728.000000  26728.000000   26728.000000  26728.000000   \n",
       "mean       0.723661      0.712249      0.583433       0.162975      0.624626   \n",
       "std        0.447195      0.452723      0.492999       0.369350      0.902864   \n",
       "min        0.000000      0.000000      0.000000       0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000       0.000000      0.000000   \n",
       "50%        1.000000      1.000000      1.000000       0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000       0.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000       1.000000      3.000000   \n",
       "\n",
       "         Is_Popular     Age_Value        Gender          dark        exotic  \\\n",
       "count  26728.000000  26728.000000  26728.000000  26728.000000  26728.000000   \n",
       "mean       0.188043    794.037227      0.524656      0.317532      0.099559   \n",
       "std        0.390753   1082.374554      0.499401      0.465525      0.299416   \n",
       "min        0.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000     60.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000    365.000000      1.000000      0.000000      0.000000   \n",
       "75%        0.000000   1095.000000      1.000000      1.000000      0.000000   \n",
       "max        1.000000   7300.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              light       neutral    Is_Weekday   time_of_day          hour  \\\n",
       "count  26728.000000  26728.000000  26728.000000  26728.000000  26728.000000   \n",
       "mean       0.289547      0.293363      0.695825      1.144904     12.767996   \n",
       "std        0.453560      0.455312      0.460066      0.951823      9.471827   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "50%        0.000000      0.000000      1.000000      2.000000     18.000000   \n",
       "75%        1.000000      1.000000      1.000000      2.000000     21.000000   \n",
       "max        1.000000      1.000000      1.000000      2.000000     23.000000   \n",
       "\n",
       "       Outcome_Label  \n",
       "count   26728.000000  \n",
       "mean        0.402911  \n",
       "std         0.490492  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classLabels = LabelEncoder()\n",
    "for c in np.unique(df['Outcome']):\n",
    "    print c, df.Outcome[df.Outcome == c].count()\n",
    "\n",
    "# given such a low representation for died and euthanasia, it would be better to make this a binary outcome\n",
    "# we will group all other values except adoption\n",
    "\n",
    "# grab a new dataframe\n",
    "df_alg = df[['Is_Fixed','Has_Name','Type','Is_Aggressive','Size','Is_Popular','Age_Value',\n",
    "             'Gender','dark','exotic','light','neutral','Is_Weekday','time_of_day','hour','Outcome_Label']]\n",
    "\n",
    "df_alg.loc[:,'Outcome_Label'] = df.loc[:,'Outcome'].apply(lambda x: 1 if x == 'Adoption' else 0)\n",
    "df_alg.loc[:,'Is_Fixed'] = df.loc[:,'Is_Fixed'].apply(lambda x: 1 if x == 'Fixed' else 0)\n",
    "\n",
    "# now let's line up the rest of our features to get ready for logistic regression and SVM\n",
    "df_alg.loc[:,'Type'] = df.loc[:,'Type'].apply(lambda x: 1 if x == 'Dog' else 0)\n",
    "df_alg.loc[:,'Gender'] = df.loc[:,'Gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
    "\n",
    "#Using Age instead: df_alg.loc[:,'age_range'] = classLabels.fit_transform(df.loc[:,'age_range'])\n",
    "df_alg.loc[:,'Size'] = classLabels.fit_transform(df.loc[:,'Size'])\n",
    "df_alg.loc[:,'Age_Value'] = df.loc[:, 'Age_Value']\n",
    "df_alg.loc[:, :] = df_alg.astype(np.int64)\n",
    "\n",
    "print df_alg.shape\n",
    "df_alg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that we have a binary outcome we're interested in, we'll go ahead and fit a logistic regression model to the full feature set we just created and perform model selection on the data. (this is assuming we don't create a validation set from our training data as we haven't gotten there yet on lesson plan). \n",
    "\n",
    "However, in order to test effectively and ensure class balances in each split, we will use StratifiedShuffleSplit from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "scl = StandardScaler()\n",
    "lr = LogisticRegression()\n",
    "X_train = df_alg.values[:,:-1] # features\n",
    "y_train = df_alg.values[:,-1] # target, binary\n",
    "\n",
    "cv_object =  StratifiedShuffleSplit(y=y_train,\n",
    "                                    n_iter=5,\n",
    "                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have included an hour feature, it would be best to scale our data. Hour's range is 24, while the second largest range is 3, for age_range. Thus we will standardize our data appropriately to represent weights correctly and help us choose which variables are the most important in our model. Unfortunately, we cannot reduce dimensionality in this case with standard methods like PCA and LDA because of such a categorical heavy dataset.\n",
    "\n",
    "Let's take a quick look at our accuracy on 3 shuffles with standardized data using an out of the box logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74130191  0.72839506  0.72989151  0.73419379  0.73400673]\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(lr, scl.fit_transform(X_train), y=y_train, cv=cv_object, verbose= 0) # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check out our accuracy across penalty and cost parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "costs = np.arange(0.001,10,0.5)\n",
    "\n",
    "l2s = []\n",
    "l1s = []\n",
    "for i in costs:\n",
    "    for j in ['l1','l2']:\n",
    "        lr2 = LogisticRegression(penalty = j, C = i)\n",
    "        accuracies = np.average(cross_val_score(lr2, scl.fit_transform(X_train), y=y_train, cv=cv_object, verbose= 0))\n",
    "        if j == 'l1':\n",
    "            l1s.append(accuracies)\n",
    "        else:\n",
    "            l2s.append(accuracies)\n",
    "# best accuracy @ cost of zero (max reg for logit) using l2 regularlization\n",
    "plt.plot(costs, l2s)\n",
    "plt.plot(costs, l1s)\n",
    "plt.legend(['l2','l1'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74% on our training set is not great, but it's better than random selection. That being said, we can definitely tune our model to understand how regularlization can help (or hinder) or classification accuracy. It looks as if L1 (LASSO) penalty is our optimal parameter setting for our training set. This will give us sparseness in our parameters, allowing us to simplify our model moreso than L2 (ridge). However, we need to first use our validation set to confirm our findings. \n",
    "\n",
    "First, let's take a look at our accuracy rate again, this time using five fold stratified cross validation and the number of observations in our data set. Let's look at our learning curve and the variability associated with our training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# playing with pipelines, adapted from Rauschka\n",
    "from sklearn.learning_curve import learning_curve # uses stratified k fold by default\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),('clf', LogisticRegression(penalty='l1'))]) # set up pipeline\n",
    "                                                                                          # automatically scale\n",
    "train_sizes, train_scores, valid_scores = learning_curve(estimator=pipe_lr, \n",
    "                                                         X = X_train, \n",
    "                                                         y = y_train, \n",
    "                                                         train_sizes = np.linspace(0.1, 1.0, 10) ,# spacing of samples, \n",
    "                                                         cv = 5, # 5 fold\n",
    "                                                         n_jobs = -1) # use all processors\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "plt.plot(train_sizes, train_mean, c='blue', marker='o',  markersize=5, label='training accuracy')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.1, color='blue')\n",
    "plt.plot(train_sizes, valid_mean, c='green', marker='s', linestyle='--',  markersize=5, label='validaiton accuracy')\n",
    "plt.fill_between(train_sizes, valid_mean + valid_std, valid_mean - valid_std, alpha=0.1, color='green')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that we do not generalize well to our validation set in our training data when the number of samples is low -- we see high variance and overfitting. Validation lags behind training accuracy on average until about 21,000 samples. There is better generalization with a larger number of samples. As expected, the validation result does have a higher variance in this case, but in general, more samples benefits the accuracy of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tune the critical hyperparameters of our Logistic Regression model: regularlization cost and penalty. We'll interactively test the parameters below and then visualize optimal cost and penalties for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets as wd\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def lr_explore(cost, penalty):\n",
    "    lr = LogisticRegression(penalty=penalty, C=cost, class_weight=None) \n",
    "    accuracies = cross_val_score(lr, scl.fit_transform(X_train), y_train, cv=cv_object) \n",
    "    print 'Average Accuracy:', np.average(accuracies)\n",
    "    \n",
    "wd.interact(lr_explore, cost=(0.001,10.0,0.05), penalty =['l1','l2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After confirming l1 penalty with a large amount of regularlization is optimal for fitting our model let's take a look directly at a visual of our regularlization parameter C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a look at C levels and associated accuracy on a stratified 5 fold CV\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, valid_scores = validation_curve(\n",
    "                                             estimator = pipe_lr,\n",
    "                                             X = X_train,\n",
    "                                             y = y_train,\n",
    "                                             param_name = 'clf__C', # vary the C parameter directly\n",
    "                                             param_range = param_range,\n",
    "                                             cv = 5\n",
    "                                             )\n",
    "train_mean = np.mean(train_scores,axis=1)\n",
    "train_std = np.std(train_scores,axis=1)\n",
    "valid_mean = np.mean(valid_scores,axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "plt.plot(param_range, train_mean, color='blue', marker='o', markersize=5,label='training accuracy')\n",
    "plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha = 0.1, color='blue')\n",
    "plt.plot(param_range, valid_mean, color='green', marker='s', linestyle='--', markersize=5,label='validation accuracy')\n",
    "plt.fill_between(param_range, valid_mean + valid_std, valid_mean - valid_std, alpha = 0.1, color='green')\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Param C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large amount of regularlization, we can see that our validation and training sets are almost identical. This indicates we are NOT overfitting our model with a significant C hyperparameter using LASSO (l1 penalty). Therefore, it's optimal we move forward with l1 and a significant regularlization. \n",
    "\n",
    "Note: sklearn's C parameter is an INVERSE regularlization parameter for logistic regression, the lower, the MORE regularlization in the model.\n",
    "\n",
    "Now, let's repeat our 80/20 split on our training set using shuffles to see if we've improved based on our research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_final = LogisticRegression(penalty='l1', C=0.001)\n",
    "accuracies = cross_val_score(lr_final, scl.fit_transform(X_train), y_train, cv=cv_object) \n",
    "print np.average(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very slight, however, an improvement! Now that we've performed model selection on our training set by dividing it into subtraining and validation sets, let's fit the entire training set and take a look at our coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_final.fit(scl.fit_transform(X_train),y_train)\n",
    "weights = pd.Series(lr_final.coef_[0], index=df_alg.columns[:-1])\n",
    "weights.plot(kind='bar')\n",
    "plt.show()\n",
    "# LASSO results show we can eliminate pretty much everything outside of is_fixed and age_range (woot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Logistic Regression Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our model is only around 74% accurate on our validation dataset, we have obvious opportunities to simplify. The standardized data has resulted in a sparse coefficient array for our Logistic Regression model. This is one of the main benefits of utilizing a LASSO approach. \n",
    "\n",
    "Is_Fixed (0.91) is obviously the most important attribute in determining whether or not an animal is adopted. If an animal is fixed, it's more likely to be adopted. Age (-.36), as well, is quite important. The younger, the more likely to be adopted.\n",
    "\n",
    "These variables, from experience and domain knowledge, were expected to be highly important. Spaying and neutering is cost prohibitive for some consumers, and the average adoptor is almost always interested in younger animals. Further, most owners do no want animals who are capable of reporoducing. Age is important because of potential health risks and maximum anticipated time with the animal. However, these assumptions are purely anecdotal and would need to be tested themselves. That being said, the ultimate conclusion given the result above is not surprising. \n",
    "\n",
    "What is interesting, however, is the lack of importance when it comes to size. This could be a result of the binarization process, in which size categories were created and may need to be revisited. \n",
    "\n",
    "The business result confirms our initial suspicions: fix the animals, name them and make them available on the weekend to maximize the chance of adoption. Our business partners cannot do anything about age.\n",
    "\n",
    "The analytical result allows us to move forward with a two feature model, including age_range and Is_Fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_reduced = df_alg[['Is_Fixed','Age_Value']].values\n",
    "lr_reduced = LogisticRegression(penalty='l1', C=0.01)\n",
    "lr_reduced.fit(X_reduced, y_train)\n",
    "accuracies = cross_val_score(lr_final, scl.fit_transform(X_train), y_train, cv=cv_object)\n",
    "print(np.average(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  matplotlib.colors import ListedColormap # credit: Sebastian Rauschka: Python Machine Learning\n",
    "def plot_decision_regions(X,y,classifier,test_idx=None,resolution=0.02):\n",
    "    # set up marker generator and color map\n",
    "    markers = ('s','x','o','^','v')\n",
    "    colors = ('red','blue','lightgreen','gray','cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # plot decision surface\n",
    "    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max() +1\n",
    "    x2_min, x2_max = X[:,1].min() - 1, X[:,1].max() +1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1,xx2,Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(),xx2.max())\n",
    "    \n",
    "    # plot all samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y==cl,0], y=X[y==cl,1],\n",
    "                   alpha=0.8,c=cmap(idx),\n",
    "                   marker=markers[idx],label=cl)\n",
    "        \n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "        plt.scatter(X_test[:,0],X_test[:,1],c='',\n",
    "                   alpha=1.0,linewidths=1,marker='o',\n",
    "                   s=55,label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_regions(scl.fit_transform(X_reduced), y_train, lr_reduced)\n",
    "plt.xlabel('Is_Fixed')\n",
    "plt.ylabel('Age_Range')\n",
    "plt.title('Classification Regions: Is_Fixed and Age_Range')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision regions visually show that fixed, young animals are adopted. While older animals have less of a chance to be adopted, even if they are fixed. \n",
    "\n",
    "We still maintain 74% accuracy, after reducing the amount of features in our dataset to only two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74% accuracy is better than random chance, however, our classification model can likely be improved by gathering critical features, such as pre-existing medical conditions, if the animal has reproduced before and solidifying whether or not an animal truly is aggressive, instead of using arbitrary lists to identify aggressive breeds. We believe this would push our model accuracy over 80% and provide a more holistic conclusion on what factors drive adoption for our business partners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take another route: support vector machines. We will fit both linear and non-linear support vector machines to the data and compare this methodology with logistic regression.\n",
    "\n",
    "We will use sklearn's SVC Support Vector Machine classifier to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Using SVM Linear Kernel\n",
    "We will use linear support vector which should give similar results to logistic regression as we are classifying on a binary class outcome.\n",
    "\n",
    "Running the training only once, with 80% of data for training, and 20% of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will take some time. Use stochasitc gradient descent instead\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "test_cases = int(len(df_alg.index) * 0.2)\n",
    "\n",
    "x_train = df_alg.values[:-test_cases,:-1] # features\n",
    "x_test = df_alg.values[-test_cases:,:-1] # features\n",
    "y_train = df_alg.values[:-test_cases,-1] # target, binary\n",
    "y_test = df_alg.values[-test_cases:,-1] # target, binary\n",
    "\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',n_jobs = -1, n_iter=1000)\n",
    "model.fit(scl.fit_transform(x_train),y_train)\n",
    "\n",
    "yhat = model.predict(scl.transform(x_test))\n",
    "score = accuracy_score(yhat, y_test)\n",
    "\n",
    "print(\"Accuracy: %s\" % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Weights\n",
    "With Linear Kernel, we are able to leverage coefficients of the support vector to determine the relative weights of the explanatory variables. Here, we plot out the relative weights of each of the features used in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = pd.Series(model.coef_[0], index=df_alg.columns[:-1])\n",
    "weights.plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Insights\n",
    "\n",
    "As in logistic regression, the same same features were chosen by the support vector as more significant in the SVM model trained from the data.\n",
    "\n",
    "In particular, whether or not the animal is fixed has the most significance in terms of adoption potential, followed by the age of the animal, if it has a name, and then whether the breed is aggressive.\n",
    "\n",
    "The hour, time of day, and whether or not the animal was brought in on the weekend also have significant impact on the adoptability of the animal, though not attributes of the animal, per se."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM RBF Kernel\n",
    "\n",
    "Now, we will use the Radial Basis Function Kernel to seek improvement here on model accuracy over the strict linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC(kernel = 'rbf')\n",
    "model.fit(scl.fit_transform(x_train),y_train)\n",
    "\n",
    "yhat = model.predict(scl.transform(x_test))\n",
    "score = accuracy_score(yhat, y_test)\n",
    "\n",
    "print(\"Accuracy: %s\" % (score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Models: Cats vs. Dogs\n",
    "Rather than incorporating only as a feature in a single model, let's break up the datasets into cats and dogs and then build separate models and compare the accuracies of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.loc[:,'Type'].apply(lambda x: 1 if x == 'Dog' else 0)\n",
    "dogs = df_alg[df_alg.Type == 1]\n",
    "cats = df_alg[df_alg.Type == 0]\n",
    "\n",
    "print np.shape(dogs) #11134\n",
    "print np.shape(cats) #15594"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dogs\n",
    "\n",
    "Now, let's train RBF kernel SVM for Dogs only and determine the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = dogs.values[:,:-1] # features\n",
    "y = dogs.values[:,-1] # target, binary\n",
    "\n",
    "test_cases = int(len(dogs.index) * 0.2)\n",
    "\n",
    "x_train = dogs.values[:-test_cases,:-1] # features\n",
    "x_test = dogs.values[-test_cases:,:-1] # features\n",
    "y_train = dogs.values[:-test_cases,-1] # target, binary\n",
    "y_test = dogs.values[-test_cases:,-1] # target, binary\n",
    "\n",
    "model = svm.SVC(kernel = 'rbf')\n",
    "model.fit(scl.fit_transform(x_train),y_train)\n",
    "\n",
    "yhat = model.predict(scl.transform(x_test))\n",
    "score = accuracy_score(yhat, y_test)\n",
    "\n",
    "print(\"Accuracy: %s\" % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower accuracy score for dogs indicates there is additional work here to bring the model to parity to that of cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cats\n",
    "\n",
    "Now, let's train RBF kernel SVM for Cats only and determine the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = cats.values[:,:-1] # features\n",
    "y = cats.values[:,-1] # target, binary\n",
    "\n",
    "test_cases = int(len(dogs.index) * 0.2)\n",
    "\n",
    "x_train = cats.values[:-test_cases,:-1] # features\n",
    "x_test = cats.values[-test_cases:,:-1] # features\n",
    "y_train = cats.values[:-test_cases,-1] # target, binary\n",
    "y_test = cats.values[-test_cases:,-1] # target, binary\n",
    "\n",
    "model = svm.SVC(kernel = 'rbf')\n",
    "model.fit(scl.fit_transform(x_train),y_train)\n",
    "\n",
    "yhat = model.predict(scl.transform(x_test))\n",
    "score = accuracy_score(yhat, y_test)\n",
    "\n",
    "print(\"Accuracy: %s\" % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of predicting adoption outcome of cats with 82% accuracy is quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Advantages/Disadvantages\n",
    "\n",
    "The logistic regression and the linear support vector models offer very similar results in terms accuracy of the models.\n",
    "\n",
    "While logistic regression was relatively quick, using SVM with the linear kernel was very slow, so we opted instead to use a version of linear SVM that employed stochastic gradient descent. With this approach, even 1000 itereations of SGD were performed quickly, proving much more efficient but building a model with the same accuracy as the regular linear SVM.\n",
    "\n",
    "Both logistic regression and linear SVM have the benefit that the coefficients of the features provide meaningful insight into the logic of the model, and are interpretable as the relative weights (either positive or negative) that each feature has on the prediction of adoption.\n",
    "\n",
    "While SVM using the RBF kernel provides better overall accuracy, it sacrifices the ability to leverage the coefficients of the support vector to derive any direct interpretability of relative weight of the individual features within the RBF model.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
